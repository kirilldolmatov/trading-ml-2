{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7020925e-97f9-4aad-8b02-270da4c54360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ad683-4c00-4e5a-9112-dce82b9bc289",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Запускать командой Run All Cells!\n",
    "\n",
    "Получаем три последние новости по тикеру с сайта Финам, с помощью ML-модели выделяем главное, отображаем результат.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176784d-ac86-4848-b3be-e51a35484f87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35755b61-c008-4628-bee3-376bb9b906f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import razdel\n",
    "import spacy\n",
    "\n",
    "# Список частей речи, которые мы не хотим считать значимыми.\n",
    "# Подбирался на глаз.\n",
    "BAD_POS = (\"PREP\", \"NPRO\", \"CONJ\", \"PRCL\", \"NUMR\", \"PRED\", \"INTJ\", \"PUNCT\", \"CCONJ\", \"ADP\", \"DET\", \"ADV\")\n",
    "\n",
    "# Загрузка модели для частеречной разметки.\n",
    "spacy_model = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "\n",
    "# Метод для разбиения текста на предложения.\n",
    "def sentenize(text):\n",
    "    return [s.text for s in razdel.sentenize(text)]\n",
    "\n",
    "\n",
    "# Метод для токенизации предложения.\n",
    "def tokenize_sentence(sentence):\n",
    "    sentence = sentence.strip().replace(\"\\xa0\", \"\")\n",
    "    tokens = [token.lemma_ for token in spacy_model(sentence) if token.pos_ not in BAD_POS]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Метод для токенизации всего текста.\n",
    "def tokenize_text(text):\n",
    "    all_tokens = []\n",
    "    for sentence in sentenize(text):\n",
    "        all_tokens.extend(tokenize_sentence(sentence))\n",
    "    return all_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5b5e9-9df5-448f-9578-b5d36bdb0f4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5a66c7f-1dc9-4986-b055-8ac3d58a2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.linalg import eig\n",
    "from tokenization import *\n",
    "\n",
    "def text_rank_preprocessing(sentence):\n",
    "    return tokenize_sentence(sentence)\n",
    "\n",
    "def text_rank_similarity(tokens1, tokens2):\n",
    "    intersection_size = sum(tokens2.count(w) for w in tokens1)\n",
    "    if intersection_size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if len(tokens1) <= 1 and len(tokens2) <= 1:\n",
    "        return intersection_size\n",
    "\n",
    "    assert len(tokens1) > 0 and len(tokens2) > 0\n",
    "    norm = math.log(len(tokens1)) + math.log(len(tokens2))\n",
    "    return intersection_size / norm\n",
    "\n",
    "\n",
    "class TextRankSummarizer:\n",
    "    \"\"\"\n",
    "    TextRank.\n",
    "    Основано на: https://github.com/miso-belica/sumy/blob/main/sumy/summarizers/text_rank.py\n",
    "    Оригинальная статья: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            damping=0.85,\n",
    "            epsilon=1e-4,\n",
    "            niter=100,\n",
    "            preprocessing_function=text_rank_preprocessing,\n",
    "            similarity_function=text_rank_similarity,\n",
    "            verbose=False\n",
    "    ):\n",
    "        self.damping = damping\n",
    "        self.epsilon = epsilon\n",
    "        self.niter = niter\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.similarity_function = similarity_function\n",
    "        self.threshold = None\n",
    "        self.verbose = True\n",
    "\n",
    "    def __call__(self, text, target_sentences_count):\n",
    "        original_sentences = sentenize(text)\n",
    "        sentences = [self.preprocessing_function(s) for s in original_sentences]\n",
    "\n",
    "        graph = self._create_graph(sentences)\n",
    "        graph = self._apply_threshold(graph)\n",
    "        # if self.verbose:\n",
    "        #     plt.figure(figsize=(15, 10))\n",
    "        #     sns.heatmap(graph, annot=True, fmt=\".2f\").set_title(\"Матрица схожести предложений\")\n",
    "        norm_graph = self._norm_graph(graph)\n",
    "        ranks = self._iterate(norm_graph)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Значимости: {}\".format(ranks))\n",
    "\n",
    "            # Можно считать PageRank библиотечными методами.\n",
    "            # При запуске на оригинальном графе должно быть то же самое.\n",
    "            nx_graph = nx.from_numpy_array(graph)\n",
    "            indices = list(range(len(sentences)))\n",
    "            nx_ranks = nx.pagerank(nx_graph)\n",
    "            nx_ranks = [ranks[i] for i in indices]\n",
    "            assert np.all(np.isclose(nx_ranks, ranks))\n",
    "            print(\"Проверка через NetworkX в порядке!\")\n",
    "\n",
    "            # Можно считать через честный метод степенных итераций над\n",
    "            # модифицированной матрицей. Должно быть то же самое.\n",
    "            random_transitions = np.full(graph.shape, 1.0 / len(graph))\n",
    "            full_matrix = (1.0 - self.damping) * random_transitions + self.damping * norm_graph\n",
    "            pm_ranks = self._power_method(full_matrix)\n",
    "            assert np.all(np.isclose(pm_ranks, ranks))\n",
    "            assert np.all(np.isclose(np.dot(full_matrix.T, pm_ranks), pm_ranks, atol=self.epsilon))\n",
    "            print(\"Проверка через метод степенных итераций в порядке!\")\n",
    "\n",
    "            # А ещё можно через собственные вектора.\n",
    "            # Только они могут отличаться на константный множитель из-за нормировки.\n",
    "            vals, vecs = eig(full_matrix.T, left=False, right=True)\n",
    "            eig_ranks = vecs[:, vals.argmax()]\n",
    "            assert np.all(np.isclose(np.dot(full_matrix.T, eig_ranks), eig_ranks, atol=self.epsilon))\n",
    "            multiplier = ranks[0] / eig_ranks[0]\n",
    "            eig_ranks *= multiplier\n",
    "            assert np.all(np.isclose(eig_ranks, ranks, atol=self.epsilon * 100))\n",
    "            print(\"Проверка через собственные вектора в порядке!\")\n",
    "\n",
    "        indices = list(range(len(sentences)))\n",
    "        indices = [idx for _, idx in sorted(zip(ranks, indices), reverse=True)]\n",
    "        indices = indices[:target_sentences_count]\n",
    "        indices.sort()\n",
    "        return \" \".join([original_sentences[idx] for idx in indices])\n",
    "\n",
    "    def set_sim_function(self, func):\n",
    "        self.similarity_function = func\n",
    "\n",
    "    def set_preprocessing_function(self, func):\n",
    "        self.preprocessing_function = func\n",
    "\n",
    "    def set_threshold(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def _create_graph(self, sentences):\n",
    "        \"\"\" Сборка изначального графа схожостей \"\"\"\n",
    "        sentences_count = len(sentences)\n",
    "        graph = np.zeros((sentences_count, sentences_count))\n",
    "        for sentence_num1, sentence1 in enumerate(sentences):\n",
    "            for sentence_num2 in range(sentence_num1, sentences_count):\n",
    "                sentence2 = sentences[sentence_num2]\n",
    "                sim = self.similarity_function(sentence1, sentence2)\n",
    "                graph[sentence_num1, sentence_num2] = sim\n",
    "                graph[sentence_num2, sentence_num1] = sim\n",
    "        return graph\n",
    "\n",
    "    def _apply_threshold(self, graph):\n",
    "        \"\"\" Обрезка графа по порогу, понадобится в LexRank \"\"\"\n",
    "        if self.threshold is None:\n",
    "            return graph\n",
    "        graph[graph < self.threshold] = 0.0\n",
    "        return graph\n",
    "\n",
    "    def _norm_graph(self, graph):\n",
    "        \"\"\"\n",
    "        Нормировка по строкам, потому что ниже p_vector - вектор, а не столбец.\n",
    "        Если бы p_vector был столбцом, надо было бы нормировать по столбцам.\n",
    "        \"\"\"\n",
    "        norm = graph.sum(axis=1)[:, np.newaxis]\n",
    "        norm_graph = graph / (norm + 1e-7)\n",
    "        assert np.isclose(np.sum(norm_graph[0, :]), 1.0)\n",
    "        assert np.all(np.isclose(norm_graph.sum(axis=1), np.ones((graph.shape[0],))))\n",
    "        return norm_graph\n",
    "\n",
    "    def _iterate(self, matrix):\n",
    "        sentences_count = len(matrix)\n",
    "        iter = 0\n",
    "        lambda_val = 0.1\n",
    "        p_vector = np.full((sentences_count,), 1.0 / sentences_count)\n",
    "        random_transitions = np.full((sentences_count,), 1.0 / sentences_count)\n",
    "\n",
    "        transposed_matrix = matrix.T\n",
    "        while iter < self.niter and lambda_val > self.epsilon:\n",
    "            next_p = (1.0 - self.damping) * random_transitions + self.damping * np.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = np.linalg.norm(np.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "            iter += 1\n",
    "        return p_vector\n",
    "\n",
    "    def _power_method(self, matrix):\n",
    "        sentences_count = len(matrix)\n",
    "        iter = 0\n",
    "        lambda_val = 0.1\n",
    "        p_vector = np.full((sentences_count,), 1.0 / sentences_count)\n",
    "\n",
    "        transposed_matrix = matrix.T\n",
    "        while iter < self.niter and lambda_val > self.epsilon:\n",
    "            next_p = np.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = np.linalg.norm(np.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "            iter += 1\n",
    "        return p_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3cd77-0c6e-4a92-8b69-14928b8b6da0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Получение новостей с сайта Финам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67b3b949-0527-4820-a944-1f65b9c7cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, elem) -> None:\n",
    "        self.link = elem.get_attribute('href')\n",
    "        span_elems = elem.find_elements(By.TAG_NAME, 'span')\n",
    "        self.date = span_elems[0].text\n",
    "        self.author = span_elems[1].text if len(span_elems) > 1 else ''\n",
    "        self.text = ''\n",
    "        self.title = ''\n",
    "\n",
    "class FinamNewsParser:\n",
    "    def __init__(self) -> None:\n",
    "        user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    def collect_news(self, ticker, start = None, end = None, maxCount = None):\n",
    "        template_url = 'https://www.finam.ru/quote/moex/{}/publications/'\n",
    "        url = template_url.format(ticker)\n",
    "    \n",
    "        if not start or not end:\n",
    "            self.driver.get(url)\n",
    "        else:\n",
    "            url +=  \"{}/{}/{}\".format('date',  start, end)\n",
    "            self.driver.get(url)\n",
    "            stop = False\n",
    "            #кликаем кнопочку \"Загрузить еще\", пока не получим все новости за период\n",
    "            while not stop:\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//span[(starts-with(@class, \"pointer\")) and (contains(@class, \"cl-blue\"))]')))\n",
    "                    self.driver.execute_script(\"finfin.local.plugin_block_item_publication_list_filter_date.loadMore(this);\")\n",
    "                except:\n",
    "                    stop = True\n",
    "\n",
    "        print(\"Getting news from:  {}\".format(url))\n",
    "        links_section = self.driver.find_element(By.ID, 'finfin-local-plugin-block-item-publication-list-filter-date-content')\n",
    "        a_elems = links_section.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "        articles = list(map(lambda elem: Article(elem), a_elems))\n",
    "\n",
    "        for id, article in enumerate(articles):\n",
    "            if maxCount is not None and id == maxCount:\n",
    "                return articles[:maxCount]\n",
    "    \n",
    "            self.driver.get(article.link)\n",
    "            try:\n",
    "                title_section = self.driver.find_element(By.TAG_NAME, 'h1')\n",
    "                article.title = title_section.text\n",
    "\n",
    "                text_section = self.driver.find_element(\n",
    "                    By.XPATH, \n",
    "                    '//div[(starts-with(@class, \"finfin-local-plugin-publication-item-item-\")) and (contains(@class, \"-text\"))]'\n",
    "                )\n",
    "                \n",
    "                p_elems = text_section.find_elements(By.TAG_NAME, 'p')\n",
    "                p_elems_text = list(map(lambda elem: elem.text, p_elems))\n",
    "\n",
    "                if len(p_elems_text):\n",
    "                    article.text = ' '.join(p_elems_text)\n",
    "            except:\n",
    "                print('Couldnt parse article from href: {}'.format(article.link))\n",
    "        \n",
    "        return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349a34e-488a-4fae-9f99-40b9214ac66f",
   "metadata": {},
   "source": [
    "## Обработка новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9c608-9e8d-43f4-9ccc-ddd6c322aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = FinamNewsParser()\n",
    "news = parser.collect_news('sber', maxCount=3)\n",
    "\n",
    "if len(news) == 0:\n",
    "    print(\"Мы не смогли найти новости по вашей компании. Давайте поробуем другую.\")\n",
    "\n",
    "print(\"Краткий пересказ последних новостей компании, которые нам удалось найти.\")\n",
    "\n",
    "string_builder = []\n",
    "result = []\n",
    "for article in news:\n",
    "    text_rank = TextRankSummarizer()\n",
    "    summary = text_rank(article.text, 3)\n",
    "    article.summary = summary\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame([t.__dict__ for t in news ])\n",
    "\n",
    "df[['title', 'text', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170dcd77-1910-4be4-bdbb-d2028c665969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
